{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.translate.metrics import alignment_error_rate\n",
    "from nltk.metrics.scores import (precision, recall)\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA\n",
    "file = open('./train.en', 'r')\n",
    "train_en = file.read()\n",
    "raw_sentences_train_en = train_en.split(\"\\n\")\n",
    "\n",
    "file = open('./train.hi', 'r')\n",
    "train_hi = file.read()\n",
    "sentences_train_hi = train_hi.split(\"\\n\")\n",
    "\n",
    "file = open('./test.en', 'r')\n",
    "test_en = file.read()\n",
    "raw_sentences_test_en = test_en.split(\"\\n\")\n",
    "\n",
    "file = open('./test.hi', 'r')\n",
    "test_hi = file.read()\n",
    "sentences_test_hi = test_hi.split(\"\\n\")\n",
    "\n",
    "file = open('./dev.en.txt', 'r')\n",
    "dev_en = file.read()\n",
    "raw_sentences_dev_en = dev_en.split(\"\\n\")\n",
    "\n",
    "file = open('./dev.hi', 'r')\n",
    "dev_hi = file.read()\n",
    "sentences_dev_hi = dev_hi.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lower_case(data):\n",
    "    list_ = []\n",
    "    for sentence in data:\n",
    "        list_.append(sentence.lower())\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dev_en = make_lower_case(raw_sentences_dev_en)\n",
    "sentences_test_en = make_lower_case(raw_sentences_test_en)\n",
    "sentences_train_en = make_lower_case(raw_sentences_train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_converged(new, old, epoch):\n",
    "    epsilone = 0.00000001\n",
    "#     new = list(new.values())\n",
    "#     old = list(old.values())\n",
    "    \n",
    "#     new_val = []\n",
    "#     for (hin, ddict) in new.items():\n",
    "#         for (eng, prob) in ddict.items():\n",
    "#             new_val.append('{0:.8f}'.format(prob))\n",
    "            \n",
    "#     old_val = []\n",
    "#     for (hin, ddict) in old.items():\n",
    "#         for (eng, prob) in ddict.items():\n",
    "#             old_val.append('{0:.8f}'.format(prob))        \n",
    "\n",
    "#     for i in range(len(old_val)):\n",
    "#         if math.fabs(float(new_val[i]) - float(old_val[i])) > epsilone: \n",
    "    if epoch < 15:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_EM(en_sentences, hi_sentences):\n",
    "    \n",
    "    uni_ini = 0.00001\n",
    "    \n",
    "    translation_prob = defaultdict(lambda: float(uni_ini))\n",
    "    translation_prob_prev = defaultdict(float)\n",
    "    \n",
    "    epoch = 0\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        epoch += 1\n",
    "        print(\"epoch num:\", epoch,\"\\n\")\n",
    "        count = defaultdict(float)\n",
    "        total = defaultdict(float)\n",
    "        for index_sen, hin_sen in enumerate(hi_sentences):\n",
    "            #compute normalization\n",
    "            hin_sen_words = hin_sen.split(\" \")\n",
    "            s_total = defaultdict(float)\n",
    "            for hin_word in hin_sen_words:\n",
    "                s_total[hin_word] = 0\n",
    "                eng_sen_words = en_sentences[index_sen].split(\" \")\n",
    "                for eng_word in eng_sen_words:\n",
    "                        s_total[hin_word] += translation_prob[(hin_word, eng_word)]\n",
    "            \n",
    "            #collect counts\n",
    "            for hin_word in hin_sen_words:\n",
    "                eng_sen_words = en_sentences[index_sen].split(\" \")\n",
    "                for eng_word in eng_sen_words:\n",
    "                        count[(hin_word, eng_word)] += translation_prob[(hin_word, eng_word)]/s_total[hin_word]\n",
    "                        total[eng_word] += translation_prob[(hin_word, eng_word)]/s_total[hin_word]                   \n",
    "\n",
    "        #estimate probabilities\n",
    "        for (hin_word, eng_word) in translation_prob.keys():\n",
    "                translation_prob[(hin_word, eng_word)] = count[(hin_word, eng_word)]/total[eng_word]\n",
    "\n",
    "        if is_converged(translation_prob, translation_prob_prev, epoch) == True:\n",
    "            break\n",
    "        \n",
    "        translation_prob_prev = translation_prob\n",
    "        \n",
    "        \n",
    "    return translation_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(sentences_train_en, sentences_train_hi):\n",
    "    \n",
    "    translation_prob = perform_EM(sentences_train_en, sentences_train_hi)\n",
    "    return translation_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num: 1 \n",
      "\n",
      "epoch num: 2 \n",
      "\n",
      "epoch num: 3 \n",
      "\n",
      "epoch num: 4 \n",
      "\n",
      "epoch num: 5 \n",
      "\n",
      "epoch num: 6 \n",
      "\n",
      "epoch num: 7 \n",
      "\n",
      "epoch num: 8 \n",
      "\n",
      "epoch num: 9 \n",
      "\n",
      "epoch num: 10 \n",
      "\n",
      "epoch num: 11 \n",
      "\n",
      "epoch num: 12 \n",
      "\n",
      "epoch num: 13 \n",
      "\n",
      "epoch num: 14 \n",
      "\n",
      "epoch num: 15 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tef = train_model(sentences_train_en, sentences_train_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"./models/IBMmodel1tef_3\", tef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterations = 0\n",
    "# for ((e_word, hin_word), value) in sorted(tef.items(), key=itemgetter(1), reverse=True):\n",
    "#     if iterations < 100:\n",
    "#         print(\"{:<20}{:>20.2}\".format(\"t(%s|%s)\" %(e_word, hin_word), value))\n",
    "#     else:\n",
    "#         break\n",
    "#     iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11966619980620694\n"
     ]
    }
   ],
   "source": [
    "print(tef[('तथा','and')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hin: बल्कि आप नेत्ररोगों से भी बचे रहेंगे ।\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-66798651253f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_translations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_dev_hi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-bf3e44cf0ee3>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(dataset, tef)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtranslated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtranslated_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d735fa0679a5>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(sentence, tef, file)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmax_eng_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0meng_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtef_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "predicted_translations = test_model(sentences_dev_hi, tef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_recall_precision(predicted_translations):\n",
    "    y_pred = list(predicted_translations)\n",
    "    y_true = sentences_dev_hi\n",
    "    recall = recall(y_true, y_pred, average=None)\n",
    "    precision = precision(y_true, y_pred, average=None)\n",
    "    return recall, precision;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
