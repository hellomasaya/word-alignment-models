{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.translate.metrics import alignment_error_rate\n",
    "from nltk.metrics.scores import (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA\n",
    "file = open('./train.en', 'r')\n",
    "train_en = file.read()\n",
    "sentences_train_en = train_en.split(\"\\n\")\n",
    "\n",
    "file = open('./train.hi', 'r')\n",
    "train_hi = file.read()\n",
    "sentences_train_hi = train_hi.split(\"\\n\")\n",
    "\n",
    "file = open('./test.en', 'r')\n",
    "test_en = file.read()\n",
    "sentences_test_en = test_en.split(\"\\n\")\n",
    "\n",
    "file = open('./test.hi', 'r')\n",
    "test_hi = file.read()\n",
    "sentences_test_hi = test_hi.split(\"\\n\")\n",
    "\n",
    "file = open('./dev.en.txt', 'r')\n",
    "dev_en = file.read()\n",
    "sentences_dev_en = dev_en.split(\"\\n\")\n",
    "\n",
    "file = open('./dev.hi', 'r')\n",
    "dev_hi = file.read()\n",
    "sentences_dev_hi = dev_hi.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_converged(new, old, epoch):\n",
    "    epsilone = 0.0000000001\n",
    "    new = list(new.values())\n",
    "    old = list(old.values())\n",
    "    \n",
    "    for i in range(len(old)):\n",
    "        if math.fabs(new[i]-old[i]) > epsilone: \n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_EM(en_sentences, hi_sentences):\n",
    "    \n",
    "    translation_prob = defaultdict(float)\n",
    "    translation_prob_prev = defaultdict(float)\n",
    "    uni_ini = 0.00001\n",
    "    \n",
    "    epoch = 0\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        epoch += 1\n",
    "        print(\"epoch num:\", epoch,\"\\n\")\n",
    "        count = defaultdict(float)\n",
    "        total = defaultdict(float)\n",
    "        for index_sen, hin_sen in enumerate(hi_sentences):\n",
    "            #compute normalization\n",
    "            hin_sen_words = hin_sen.split(\" \")\n",
    "            s_total = defaultdict(float)\n",
    "            for hin_word in hin_sen_words:\n",
    "                s_total[hin_word] = 0\n",
    "                eng_sen_words = en_sentences[index_sen].split(\" \")\n",
    "                for eng_word in eng_sen_words:\n",
    "                    if epoch == 1:\n",
    "                        s_total[hin_word] += uni_ini\n",
    "                        translation_prob[(hin_word, eng_word)] = uni_ini\n",
    "                    else:\n",
    "                        s_total[hin_word] += translation_prob[(hin_word, eng_word)]\n",
    "            \n",
    "            #collect counts\n",
    "            for hin_word in hin_sen_words:\n",
    "                eng_sen_words = en_sentences[index_sen].split(\" \")\n",
    "                for eng_word in eng_sen_words:\n",
    "                    if epoch == 1:\n",
    "                        translation_prob[(hin_word, eng_word)] = uni_ini\n",
    "                        count[(hin_word, eng_word)] += uni_ini/s_total[hin_word]\n",
    "                        total[eng_word] += uni_ini/s_total[hin_word]\n",
    "                    else:\n",
    "                        count[(hin_word, eng_word)] += translation_prob[(hin_word, eng_word)]/s_total[hin_word]\n",
    "                        total[eng_word] += translation_prob[(hin_word, eng_word)]/s_total[hin_word]                   \n",
    "\n",
    "        #estimate probabilities\n",
    "        for (hin_word, eng_word) in translation_prob.keys():\n",
    "                translation_prob[(hin_word, eng_word)] = count[(hin_word, eng_word)]/total[eng_word]\n",
    "\n",
    "        if is_converged(translation_prob, translation_prob_prev, epoch) == True:\n",
    "            break\n",
    "        \n",
    "        translation_prob_prev = translation_prob\n",
    "        \n",
    "        \n",
    "    return translation_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(sentences_train_en, sentences_train_hi):\n",
    "    \n",
    "    translation_prob = perform_EM(sentences_train_en, sentences_train_hi)\n",
    "    return translation_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dataset, tef):\n",
    "#     tef = np.load('./models/IBMmodel1tef.npy')\n",
    "    for sentence in dataset:\n",
    "        translate_sentence(sentence, tef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, tef):\n",
    "    \n",
    "    ## take the best translation of an english word\n",
    "    ## and append to hindi sentence\n",
    "    print(\"s:\",sentence)\n",
    "    tokens = sentence.split(\" \")\n",
    "    for idx, token in enumerate(tokens):\n",
    "        print(idx)\n",
    "        max_score = -1\n",
    "        for i in range(len(tef)):\n",
    "#             print(\"search i:\", i, token.lower(), list(tef.keys())[i][1].lower())\n",
    "            if token.lower() == list(tef.keys())[i][1].lower():\n",
    "                print(\"i\", list(tef.items())[i])\n",
    "\n",
    "    \n",
    "#     max_sentence = \"\"\n",
    "    \n",
    "#     prob = get_translation_prob(end_sentence, tef)\n",
    "    \n",
    "#     if prob > max_score:\n",
    "#         max_score = prob\n",
    "#         max_sentence = poss_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num: 1 \n",
      "\n",
      "epoch num: 2 \n",
      "\n",
      "s 2\n"
     ]
    }
   ],
   "source": [
    "tef = train_model(sentences_train_en, sentences_train_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"./models/IBMmodel1tef_3\", tef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(tef)[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: but you will also be safe from eye diseases .\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "test_model(sentences_dev_en, tef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_recall_precision(predicted_translations):\n",
    "    y_pred = list(predicted_translations)\n",
    "    y_true = sentences_dev_hi\n",
    "    recall = recall(y_true, y_pred, average=None)\n",
    "    precision = precision(y_true, y_pred, average=None)\n",
    "    return recall, precision;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
